{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6903bb36-c739-4de0-8b40-376dc86c6ab4",
   "metadata": {},
   "source": [
    "---\n",
    "title: Preprocessing data\n",
    "authors:\n",
    "  - name: Tasha Snow\n",
    "    affiliations:\n",
    "      - id: umd\n",
    "        institution: University of Maryland\n",
    "        department: ESSIC\n",
    "license: Apache 2.0\n",
    "date: 2025â€‘08â€‘07\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824bf52-0fb8-43f4-94e5-0b4db80a45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q intake-stac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c8a1a8-da81-4d1f-a1c6-55d15720730f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a90f1-d213-426c-baa7-24e75499e84c",
   "metadata": {},
   "source": [
    "Once you have acquired your data, the next step is **preprocessing**â€”preparing the data for computing the retrieval and generating the final data product. Preprocessing will include one or more of the following tasks:\n",
    "\n",
    "- [Cleaning](https://realpython.com/python-data-cleaning-numpy-pandas/)\n",
    "- [Unit conversion](https://pint.readthedocs.io/en/stable/)\n",
    "- [Reprojection](https://pyproj4.github.io/pyproj/stable/)\n",
    "- [Regridding](https://xesmf.readthedocs.io/en/latest/)\n",
    "- [Normalization](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)\n",
    "\n",
    "Each of these steps helps standardize the data so it can be combined, compared, or analyzed reliably.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Preprocessing Steps\n",
    "\n",
    ":::{dropdown} ðŸ”¹ Cleaning\n",
    "**Data cleaning** is required when the dataset contains missing values, outliers, or artifacts that could bias the analysis. This might include filtering NaNs, removing physically impossible values, or masking bad pixels.\n",
    "\n",
    "To clean the data, you typically:\n",
    "- Identify invalid or missing values.\n",
    "- Mask or remove unreliable data.\n",
    "- Optionally, interpolate or fill gaps as needed.\n",
    "\n",
    "ðŸ“š [Tutorial: Data Cleaning with Pandas and NumPy (RealPython)](https://realpython.com/python-data-cleaning-numpy-pandas/)\n",
    ":::\n",
    "\n",
    ":::{dropdown} ðŸ”¹ Unit Conversion\n",
    "**Unit conversion** is needed when datasets use different physical units (e.g., Kelvin vs. Celsius, W/mÂ² vs. mW/cmÂ²) or when preparing inputs for physical equations that require standardized units.\n",
    "\n",
    "This may also require spatial integration (e.g., converting a flux to energy) to match units over time and space.\n",
    "\n",
    "ðŸ“š [Pint Documentation â€“ Units in Python](https://pint.readthedocs.io/en/stable/)\n",
    ":::\n",
    "\n",
    ":::{dropdown} ðŸ”¹ Reprojection\n",
    "**Reprojection** is required when datasets are provided in different coordinate reference systems (CRS). Working with mismatched projections can lead to spatial misalignmentâ€”features may not overlap or align correctly.\n",
    "\n",
    "To reproject data:\n",
    "- Determine the CRS of each dataset.\n",
    "- Use geospatial tools to transform to a common projection.\n",
    "\n",
    "ðŸ“š [Pyproj Documentation](https://pyproj4.github.io/pyproj/stable/)\n",
    ":::\n",
    "\n",
    ":::{dropdown} ðŸ”¹ Regridding\n",
    "**Regridding** is used when datasets have different spatial resolutions or grid layouts and need to be brought onto a common grid. For example, satellite data may be on a swath-based grid while model output is on a regular latitude-longitude grid.\n",
    "\n",
    "This step ensures datasets are co-located in space and is critical for any pixel-wise comparison or combination.\n",
    "\n",
    "ðŸ“š [xESMF for Regridding](https://xesmf.readthedocs.io/en/latest/)\n",
    ":::\n",
    "\n",
    ":::{dropdown} ðŸ”¹ Normalization\n",
    "**Normalization** rescales data so that it is on a consistent numerical scale, especially important when combining variables with different units or orders of magnitude as inputs to a model (e.g., temperature vs. elevation vs. reflectance).\n",
    "\n",
    "For example, normalizing input features before passing them into a machine learning model helps ensure each variable contributes proportionally.\n",
    "\n",
    "ðŸ“š [scikit-learn Preprocessing: Normalization and Scaling](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    ":::\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3448c-bad7-4217-af5f-fcf14b22f98e",
   "metadata": {},
   "source": [
    "In the **Sea Surface Temperature (SST)** workflow demonstrated in this cookbook, we will be using all of the preprocessing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20258664-94b9-41b1-8b5f-147bacf22310",
   "metadata": {},
   "source": [
    "## Read in Landsat thermal data\n",
    "Let's begin by reading in the data we acquired previously in the Data Access notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942bb11-25ee-4fa0-9568-cc9222469537",
   "metadata": {},
   "source": [
    "We read in all paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b862806-67ed-4678-ad8f-930d2490d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/jovyan/landsatproduct-cookbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e31e77-d65a-419c-8dde-424fc52c78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import boto3\n",
    "from rasterio.session import AWSSession\n",
    "import earthaccess\n",
    "import intake\n",
    "import xarray as xr\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import geopandas as gpd\n",
    "\n",
    "import SSTutils as stu\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6111e3-6b51-4d9b-a5a5-9bbd0f1ec275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the landsat STAC catalog location\n",
    "url = 'https://landsatlook.usgs.gov/stac-server'\n",
    "\n",
    "# For atm correction\n",
    "basepath = Path('/home/jovyan/Landsat_SST_algorithm')\n",
    "lsatpath = basepath / 'Data'\n",
    "atmpath = lsatpath / 'AtmCorrection'\n",
    "modout_path = lsatpath / 'MOD07_L2'\n",
    "\n",
    "WV = 'Water_Vapor'\n",
    "\n",
    "# For search and tile plot for Landsat\n",
    "satellite = 'Landsat8'\n",
    "collection = 'landsat-c2l1' # Landsat Collection 2, Level 1 - includes L8 and L9\n",
    "colnm = ['landsat:wrs_path','landsat:wrs_row']\n",
    "gjson_outfile = lsatpath / f'{satellite}.geojson'\n",
    "\n",
    "# # For scene search and plot\n",
    "\n",
    "interp = 1\n",
    "\n",
    "region = 'Cosgrove'\n",
    "\n",
    "if region=='Cosgrove':\n",
    "    SSTpath = lsatpath / 'SST/MODcalib/Cosgrove/'\n",
    "    bbox = (-103.0, -73.5, -102.0, -73.42) # LatboundsC from LandsatCalibration20201031\n",
    "    # Cosgrove full run \n",
    "    timeRange = '2021-09-01/2023-04-07'\n",
    "    # [0:1] Cosgrove bbox\n",
    "    # timeRange = '2022-11-01/2023-03-27'\n",
    "elif region=='DotsonPolynya':\n",
    "    SSTpath = lsatpath / 'SST/MODcalib/DotsonPolynya/'\n",
    "    bbox = (-113, -73.9, -111.5, -73.59) # Dotson polynya\n",
    "    # Dotson full run\n",
    "    timeRange = '2021-09-01/2023-05-31'\n",
    "    # [0:1] Dotson bbox\n",
    "    # timeRange = '2022-11-01/2023-03-27'\n",
    "elif region=='PineIslandPlume':\n",
    "    SSTpath = lsatpath / 'SST/UncalibratedSST/PineIslandPlume/'\n",
    "    bbox = (-101.98,-75.09,-101.65,-75.05) # PIG plume for analysis - 2014\n",
    "    # bbox = (-101.88,-75.23,-100.35,-74.76) # PIG ice front for analysis\n",
    "    # bbox = (-101.8,-75.23,-100.50,-74.80) # PIG 2019?\n",
    "    # PIG full run - NOT narrowed down yet\n",
    "    timeRange = '2021-09-01/2023-04-07'\n",
    "elif region=='DotsonIntercomp':\n",
    "    SSTpath = lsatpath / 'SST/Validation/DotsonIntercomp/'\n",
    "    bbox = (-113.5,-74.20,-113.17,-74.11) # Dotson plume for analysis\n",
    "    # Dotson intercomp run\n",
    "    timeRange = '2021-09-01/2023-03-31'\n",
    "elif region=='Burke':\n",
    "    SSTpath = lsatpath / 'SST/MODcalib/Burke/'\n",
    "    bbox = (-104.2,-73.81, -103.8, -73.42) # Outside Cosgrove south of Burke  \n",
    "    # Burke full run\n",
    "    timeRange = '2021-09-01/2023-04-06'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38770399-0597-40cc-b7da-901b985d3ac8",
   "metadata": {
    "tags": []
   },
   "source": [
    "We set up authentication for accessing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed492f44-a511-4e7e-90f3-36143ac6ee87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authenticate for boto S3 access, etc.\n",
    "os.environ[\"AWS_REQUEST_PAYER\"] = \"requester\"\n",
    "aws_session = AWSSession(boto3.Session(), requester_pays=True)\n",
    "\n",
    "# Setup and authenticate dask\n",
    "from dask.distributed import Client\n",
    "import logging\n",
    "client = Client(processes=True, n_workers=4, \n",
    "                threads_per_worker=1,\n",
    "                silence_logs=logging.ERROR)\n",
    "client.run(lambda: os.environ[\"AWS_REQUEST_PAYER\"] == \"requester\" )\n",
    "client\n",
    "\n",
    "# Authenticate for accessing NASA data (MODIS)\n",
    "auth = earthaccess.login(strategy=\"interactive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410165f-1321-47b8-aee8-0aae8a35b7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search for desired Landsat scenes\n",
    "items = stu.search_stac(url, collection, gjson_outfile=gjson_outfile, bbox=bbox, timeRange=timeRange)\n",
    "\n",
    "# Open stac catalog for some needed info\n",
    "catalog = intake.open_stac_item_collection(items)\n",
    "\n",
    "# Load the geojson file\n",
    "gf = gpd.read_file(gjson_outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad2ca1-f184-4835-9d1f-75b496c3e625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot tiles of all scenes found\n",
    "stu.plot_search(gf,satellite,colnm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d93e7f-596a-4ef7-9b3c-9ce04c7deb63",
   "metadata": {},
   "source": [
    "**Note the bands you would like to include are assigned by passing the bandNames parameter to landsat_to_xarray using the following codes:**\n",
    "\n",
    "'coastal', 'blue', 'green', 'red', 'nir08', 'swir16', 'swir22', 'pan', 'cirrus', 'lwir11', 'lwir12', 'qa_pixel'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d3364-3177-4984-aa20-f26d850c1611",
   "metadata": {},
   "source": [
    "## Process Landsat scenes to acquire sea surface temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5cd70-099d-4da0-87e9-6d6ac4e8c899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert bounding box to polar for checking if landsat has any data in bounding box\n",
    "source_crs = 'epsg:4326' \n",
    "target_crs = 'epsg:3031' # Coordinate system of the file\n",
    "\n",
    "sbox,checkbox = stu.lsat_reproj(source_crs,target_crs,(bbox[0],bbox[1],bbox[2],bbox[3]))\n",
    "\n",
    "# Create polygon for later cropping\n",
    "polygon = Polygon([(sbox[0][0],sbox[0][1]),(sbox[3][0],sbox[3][1]),(sbox[2][0],sbox[2][1]),(sbox[1][0],sbox[1][1])])\n",
    "\n",
    "# Create min/max boundaries for trimming image before crop_xarray to cut down on processing times\n",
    "minx, miny, maxx, maxy = polygon.bounds\n",
    "polarx = [minx, maxx]\n",
    "polary = [miny, maxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7922b-7a2f-412c-8fe6-41315ca27fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only Landsat 8 scenes\n",
    "catalog_list = [x for x in items if x.id[3]=='8']\n",
    "\n",
    "sceneid = catalog_list[0]\n",
    "print(sceneid.id)\n",
    "    \n",
    "scene = catalog[sceneid.id]\n",
    "timestr = scene.metadata['datetime'].strftime('%H%M%S')\n",
    "\n",
    "outFile = f'{SSTpath}/{sceneid.id}_{timestr}_Cel.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c57e13-c584-41c8-8c7b-7a4e5eb9eb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open all desired bands for one scene\n",
    "ls_scene = stu.landsat_to_xarray(sceneid,catalog)\n",
    "ls_scene = ls_scene.rio.write_crs(\"epsg:3031\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b30e80-081f-4668-a4ac-e9e4a5aaf6bd",
   "metadata": {},
   "source": [
    "## Masking Unwanted Pixels in Landsat Thermal Imagery\n",
    "\n",
    "In our Landsat SST algorithm, the **first preprocessing step** is to ensure that we only process **ocean pixels**.\n",
    "\n",
    "Why?  \n",
    "Thermal infrared measurements are highly sensitive to atmospheric effects, particularly **water vapor**, and cannot provide accurate surface temperature if **clouds** are present.  \n",
    "Additionally, we **donâ€™t want SST from land or ice** pixels.\n",
    "\n",
    "This means our first preprocessing task is **masking**â€”identifying and excluding pixels that shouldnâ€™t be processed.\n",
    "\n",
    "---\n",
    "\n",
    "## Sources for Pixel Classification\n",
    "\n",
    "Landsat imagery includes a **`qa_pixel` band** with bit flags that encode surface classification for each pixel.\n",
    "\n",
    "We can:\n",
    "1. **Use the `qa_pixel` band** to mask unwanted pixels (our approach here).\n",
    "2. **Replace or augment** with a machine learning classifier (e.g., neural network) for more accurate cloud detection.\n",
    "\n",
    "> **Note:** The standard Landsat cloud classification is not well-suited for detecting certain types of cloud. ML-based classifiers often outperform it for certain conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 â€“ Inspect the QA Band\n",
    "\n",
    "Letâ€™s first inspect whatâ€™s inside the QA band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce1b2a-f898-4eb5-87c1-ff7a6c0d8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ls_scene.sel(band='qa_pixel').astype('uint16')\n",
    "unique_values, counts = np.unique(qa, return_counts=True)\n",
    "print(\"Unique QA codes:\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71197ea0-e26c-464f-a8d9-1e4e281ec459",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/jovyan/landsatproduct-cookbook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c2d9b-d5a2-4529-89b2-3d47a254b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(ls_scene, cloud_mask=True, ice_mask=False, ocean_mask=False):\n",
    "    \"\"\"\n",
    "    Creates cloud, ice, and ocean masks from a Landsat scene QA band. By default, \n",
    "    clouds are labeled as 1, ice as 2, ocean as 3, and all other pixels are NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ls_scene : xarray.DataArray\n",
    "        A Landsat scene loaded with a 'qa_pixel' band (as created by `landsat_to_xarray`).\n",
    "    cloud_mask : bool, optional\n",
    "        Whether to generate the cloud mask. Default is True.\n",
    "    ice_mask : bool, optional\n",
    "        Whether to generate the ice mask. Default is False.\n",
    "    ocean_mask : bool, optional\n",
    "        Whether to generate the ocean mask. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        The same input xarray object, but with an added `\"mask\"` coordinate. \n",
    "        In that mask, cloud pixels are assigned 1, ice pixels 2, ocean pixels 3, \n",
    "        and everything else is set to NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    cloud = []\n",
    "    ocean = []\n",
    "    ice = []\n",
    "\n",
    "    qa = ls_scene.sel(band='qa_pixel').astype('uint16')\n",
    "\n",
    "    n,c = np.unique(qa, return_counts=True)\n",
    "\n",
    "    for j in range(len(n)):\n",
    "        longform = f'{n[j]:016b}'\n",
    "        if (longform[-7]=='0')|(longform[-3]=='1'): #bit 2 and 6 are for cirrus and clear sky\n",
    "            cloud.append(n[j])\n",
    "        if longform[-8:]=='11000000': #bit 6 and 7 give clear sky and water, lower bits need to be 0 \n",
    "            ocean.append(n[j])\n",
    "        if longform[-7:]=='1100000': #bit 5 and 6 give ice and clear sky \n",
    "            ice.append(n[j])\n",
    "\n",
    "    if 0 in cloud:\n",
    "        cloud.remove(0)\n",
    "    if 1 in cloud:\n",
    "        cloud.remove(1)\n",
    "\n",
    "    # mask cloud, ice, and ocean\n",
    "    if cloud_mask==True:\n",
    "        # cloud is 2\n",
    "        mask_c = xr.where(qa.isin(cloud), 1, np.nan)\n",
    "\n",
    "    if ice_mask==True:\n",
    "        mask_c = xr.where(qa.isin(ice), 2, mask_c)\n",
    "\n",
    "    if ocean_mask==True:\n",
    "        mask_c = xr.where(qa.isin(ocean), 3, mask_c)\n",
    "\n",
    "    ls_scene.coords['mask'] = (('y', 'x'), mask_c.data)\n",
    "        \n",
    "    return ls_scene\n",
    "\n",
    "##########################\n",
    "\n",
    "def normalize(array):\n",
    "    '''\n",
    "    normalize a dask array so all value are between 0 and 1\n",
    "    '''\n",
    "    array_min = array.min(skipna=True)\n",
    "    array_max = array.max(skipna=True)\n",
    "    return (array - array_min) / (array_max - array_min)\n",
    "\n",
    "##########################\n",
    "\n",
    "def search_stac(url, collection, gjson_outfile=None, bbox=None, timeRange=None, filename=None):\n",
    "    \"\"\"\n",
    "    Search a STAC API for Landsat images based on either:\n",
    "    - Bounding box and time range, or\n",
    "    - Specific filename (STAC 'id').\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    url : str\n",
    "        URL to the STAC API.\n",
    "    collection : str\n",
    "        Collection name (e.g., \"landsat-c2-l2\").\n",
    "    gjson_outfile : str or None\n",
    "        Output file to save the search result as GeoJSON (optional).\n",
    "    bbox : list or None\n",
    "        Bounding box [west, south, east, north] (optional).\n",
    "    timeRange : str or None\n",
    "        Time range in ISO format, e.g., '2021-09-01/2023-03-31' (optional).\n",
    "    filename : str or None\n",
    "        Exact filename (product ID) to search for (optional).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    item_collection : pystac.ItemCollection\n",
    "        Collection of matching STAC items.\n",
    "    \"\"\"\n",
    "    \n",
    "    api = pystac_client.Client.open(url)\n",
    "\n",
    "    if filename:\n",
    "        # Search by filename (ID)\n",
    "        search = api.search(\n",
    "            collections=[collection],\n",
    "            ids=[filename],\n",
    "        )\n",
    "        # print(f\"Searching for filename: {filename}\")\n",
    "    \n",
    "    elif bbox and timeRange:\n",
    "        # Search by bbox and timeRange\n",
    "        search = api.search(\n",
    "            bbox=bbox,\n",
    "            datetime=timeRange,\n",
    "            collections=[collection],\n",
    "        )\n",
    "        # print(f\"Searching for items in bbox {bbox} and timeRange {timeRange}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Must provide either a filename, or both bbox and timeRange.\")\n",
    "\n",
    "    items = search.item_collection()\n",
    "\n",
    "    # print(f\"Found {len(items)} item(s)\")\n",
    "\n",
    "    if gjson_outfile:\n",
    "        items.save_object(gjson_outfile)\n",
    "    \n",
    "    return items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
